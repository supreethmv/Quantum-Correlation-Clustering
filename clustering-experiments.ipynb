{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54977e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dwave.system import DWaveSampler, EmbeddingComposite\n",
    "from dimod import BinaryQuadraticModel\n",
    "\n",
    "import networkx as nx\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f98c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    normalized_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    fowlkes_mallows_score,\n",
    "    adjusted_rand_score\n",
    ")\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4a70bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ae0e43",
   "metadata": {},
   "source": [
    "## GCS-Q Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Construct the initial graph from the adjacency matrix\n",
    "def construct_graph(adj_matrix):\n",
    "    G = nx.Graph()\n",
    "    num_nodes = len(adj_matrix)\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):\n",
    "            if adj_matrix[i][j] != 0:\n",
    "                G.add_edge(i, j, weight=adj_matrix[i][j])\n",
    "    return G\n",
    "\n",
    "# Step 2: Define the function to calculate coalition value (sum of weights)\n",
    "def coalition_value(subgraph):\n",
    "    return subgraph.size(weight='weight')\n",
    "\n",
    "\n",
    "def get_qubo_matrix(W):\n",
    "    \"\"\"Computes the QUBO matrix for the Minimum Cut problem given a weight matrix W.\"\"\"\n",
    "    n = W.shape[0]  # Number of nodes\n",
    "    Q = np.zeros((n, n))  # Initialize QUBO matrix\n",
    "    \n",
    "    for i in range(n):\n",
    "        Q[i, i] = np.sum(W[i])  # Diagonal terms (degree of node)\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                Q[i, j] = -W[i, j]  # Off-diagonal terms (negative adjacency)\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# Step 3: Bipartitioning using QUBO and Quantum Annealing\n",
    "def bipartition(graph):\n",
    "    if len(graph.nodes())==1:\n",
    "       return [], [0], 0\n",
    "\n",
    "    w = nx.adjacency_matrix(graph).todense()\n",
    "    qubo = get_qubo_matrix(W = w)\n",
    "\n",
    "    bqm = BinaryQuadraticModel.from_qubo(qubo)\n",
    "\n",
    "    sampler = EmbeddingComposite(DWaveSampler(token = open('dwave-api-token.txt','r').read(), solver={'topology__type': 'pegasus'}))\n",
    "    sampleset = sampler.sample(bqm, num_reads=1000)\n",
    "    qpu_access_time = sampleset.info['timing']['qpu_access_time']\n",
    "\n",
    "    solution = sampleset.first.sample\n",
    "    partition1 = [node for node in solution if solution[node] == 1]\n",
    "    partition2 = [node for node in solution if solution[node] == 0]\n",
    "\n",
    "    return partition1, partition2, qpu_access_time\n",
    "\n",
    "def gurobi_qubo_solver_old(linear,quadratic):\n",
    "    qubo_matrix = np.zeros([len(linear),len(linear)])\n",
    "    for key,value in linear.items():\n",
    "        qubo_matrix[int(key),int(key)] = value\n",
    "    for key,value in quadratic.items():\n",
    "        qubo_matrix[int(key[0]),int(key[1])] = value/2\n",
    "        qubo_matrix[int(key[1]),int(key[0])] = value/2\n",
    "    n = qubo_matrix.shape[0]\n",
    "    model = gp.Model()\n",
    "    x = model.addVars(n, vtype=GRB.BINARY)\n",
    "    obj_expr = gp.quicksum(qubo_matrix[i, j] * x[i] * x[j] for i in range(n) for j in range(n))\n",
    "    model.setObjective(obj_expr)\n",
    "    model.setParam('OutputFlag', 0)\n",
    "    model.optimize()\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        solution = [int(x[i].X) for i in range(n)]\n",
    "        binary_string = ''.join(str(bit) for bit in solution)\n",
    "        return binary_string, model.objVal\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def gurobi_qubo_solver(qubo_matrix):\n",
    "    n = qubo_matrix.shape[0]\n",
    "    model = gp.Model()\n",
    "    x = model.addVars(n, vtype=GRB.BINARY)\n",
    "    obj_expr = gp.quicksum(qubo_matrix[i, j] * x[i] * x[j] for i in range(n) for j in range(n))\n",
    "    model.setObjective(obj_expr)\n",
    "    model.setParam('OutputFlag', 0)\n",
    "    model.optimize()\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        solution = [int(x[i].X) for i in range(n)]\n",
    "        binary_string = ''.join(str(bit) for bit in solution)\n",
    "        return binary_string, model.objVal\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Step 3: Bipartitioning using QUBO and Quantum Annealing\n",
    "def bipartition_gurobi(graph):\n",
    "    if len(graph.nodes())==1:\n",
    "       return [], [0]\n",
    "    w = nx.adjacency_matrix(graph).todense()\n",
    "    qubo = get_qubo_matrix(W = w)\n",
    "    solution_str, objective_value = gurobi_qubo_solver(qubo)\n",
    "    solution = {idx:int(bit) for idx,bit in enumerate(solution_str)}\n",
    "    partition1 = [node for node in solution if solution[node] == 1]\n",
    "    partition2 = [node for node in solution if solution[node] == 0]\n",
    "\n",
    "    return partition1, partition2\n",
    "\n",
    "\n",
    "# Step 4: Iterative GCS-Q Algorithm\n",
    "def gcs_q_algorithm(adj_matrix, qubo_solver = \"dwave\"):\n",
    "    G = construct_graph(adj_matrix)\n",
    "    grand_coalition = list(G.nodes)\n",
    "    queue = [grand_coalition]\n",
    "    CS_star = []\n",
    "    total_qpu_access_time = 0\n",
    "    total_partition_time = 0\n",
    "\n",
    "    while queue:\n",
    "        C = queue.pop(0)  # Dequeue the first coalition\n",
    "        subgraph = G.subgraph(C).copy()\n",
    "\n",
    "        # Solve the optimal split problem\n",
    "        if qubo_solver == \"dwave\":\n",
    "            t0 = time.time()\n",
    "            partition1, partition2, qpu_access_time = bipartition(subgraph)\n",
    "            partition_time = time.time() - t0\n",
    "            total_qpu_access_time += qpu_access_time\n",
    "            total_partition_time += partition_time\n",
    "        elif qubo_solver == \"gurobi\":\n",
    "            partition1, partition2 = bipartition_gurobi(subgraph)\n",
    "        partition1 = [C[subgraph_node_index] for subgraph_node_index in partition1]\n",
    "        partition2 = [C[subgraph_node_index] for subgraph_node_index in partition2]\n",
    "\n",
    "        if not partition2:  # If no meaningful split is found\n",
    "            CS_star.append(partition1)\n",
    "        elif not partition1:\n",
    "            CS_star.append(partition2)\n",
    "        else:  # If a meaningful split is found, enqueue the partitions\n",
    "            queue.append(partition1)\n",
    "            queue.append(partition2)\n",
    "    \n",
    "\n",
    "    return CS_star, total_qpu_access_time/10**6, total_partition_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d46506",
   "metadata": {},
   "source": [
    "## PAM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d1de9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_cost(distance_matrix, medoids, clusters):\n",
    "    total_cost = 0\n",
    "    for medoid, cluster in zip(medoids, clusters):\n",
    "        total_cost += np.sum(distance_matrix[cluster][:, medoid])\n",
    "    return total_cost\n",
    "\n",
    "def assign_clusters(distance_matrix, medoids):\n",
    "    clusters = [[] for _ in range(len(medoids))]\n",
    "    for i in range(distance_matrix.shape[0]):\n",
    "        distances_to_medoids = [distance_matrix[i, medoid] for medoid in medoids]\n",
    "        closest_medoid = np.argmin(distances_to_medoids)\n",
    "        clusters[closest_medoid].append(i)\n",
    "    return clusters\n",
    "\n",
    "def pam(distance_matrix, k, max_iter=100):\n",
    "    # Step 1: Initialize medoids\n",
    "    medoids = np.random.choice(distance_matrix.shape[0], k, replace=False)\n",
    "    best_medoids = medoids.copy()\n",
    "    clusters = assign_clusters(distance_matrix, medoids)\n",
    "    best_cost = calculate_total_cost(distance_matrix, medoids, clusters)\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        for medoid_idx in range(k):\n",
    "            current_medoid = medoids[medoid_idx]\n",
    "            non_medoids = [i for i in range(distance_matrix.shape[0]) if i not in medoids]\n",
    "            \n",
    "            for new_medoid in non_medoids:\n",
    "                new_medoids = medoids.copy()\n",
    "                new_medoids[medoid_idx] = new_medoid\n",
    "                new_clusters = assign_clusters(distance_matrix, new_medoids)\n",
    "                new_cost = calculate_total_cost(distance_matrix, new_medoids, new_clusters)\n",
    "                \n",
    "                if new_cost < best_cost:\n",
    "                    best_cost = new_cost\n",
    "                    best_medoids = new_medoids.copy()\n",
    "                    clusters = new_clusters\n",
    "                    \n",
    "        if np.array_equal(best_medoids, medoids):\n",
    "            break\n",
    "        else:\n",
    "            medoids = best_medoids.copy()\n",
    "    \n",
    "    return best_medoids, clusters\n",
    "\n",
    "def clusters_as_set_of_sets(clusters):\n",
    "    return [cluster for cluster in clusters]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8b1df",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03541e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_kmeans(adjacency_matrix, k, seed=None):\n",
    "    from sklearn.manifold import MDS\n",
    "    mds = MDS(n_components=2, dissimilarity='euclidean', random_state=seed)\n",
    "    coords = mds.fit_transform(adjacency_matrix)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=seed)\n",
    "    labels = kmeans.fit_predict(coords)\n",
    "    \n",
    "    clusters = [[] for _ in range(k)]\n",
    "    for idx, label in enumerate(labels):\n",
    "        clusters[label].append(idx)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af051d",
   "metadata": {},
   "source": [
    "## Agglomerative Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb0199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hierarchical(adjacency_matrix, k):\n",
    "    model = AgglomerativeClustering(n_clusters=k)\n",
    "    labels = model.fit_predict(adjacency_matrix)\n",
    "    \n",
    "    clusters = [[] for _ in range(k)]\n",
    "    for idx, label in enumerate(labels):\n",
    "        clusters[label].append(idx)\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a86130",
   "metadata": {},
   "source": [
    "## Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_spectral(adjcency_matrix, k, seed=None):\n",
    "    model = SpectralClustering(n_clusters=k, affinity='rbf', random_state=seed)\n",
    "    labels = model.fit_predict(adjcency_matrix)\n",
    "    \n",
    "    clusters = [[] for _ in range(k)]\n",
    "    for idx, label in enumerate(labels):\n",
    "        clusters[label].append(idx)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf7240",
   "metadata": {},
   "source": [
    "## DIANA (Divisive Hierarchical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "08526144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def diana(dist_matrix, k):\n",
    "    n = dist_matrix.shape[0]\n",
    "    clusters = [list(range(n))]\n",
    "\n",
    "    while len(clusters) < k:\n",
    "        # Select the cluster with the largest diameter\n",
    "        diameters = [np.max(dist_matrix[np.ix_(c, c)]) for c in clusters]\n",
    "        idx_to_split = np.argmax(diameters)\n",
    "        cluster = clusters.pop(idx_to_split)\n",
    "\n",
    "        # 1. Find point most dissimilar to all others\n",
    "        avg_dists = np.mean(dist_matrix[np.ix_([i for i in cluster], [j for j in cluster])], axis=1)\n",
    "        split_seed_idx = cluster[np.argmax(avg_dists)]\n",
    "        new_cluster = [split_seed_idx]\n",
    "        remaining = set(cluster) - {split_seed_idx}\n",
    "\n",
    "        # 2. Move points one by one to new cluster if it decreases average dissimilarity\n",
    "        for i in remaining:\n",
    "            in_old = cluster.copy()\n",
    "            in_old.remove(i)\n",
    "            old_avg = np.mean([dist_matrix[i][j] for j in in_old])\n",
    "\n",
    "            new_avg = np.mean([dist_matrix[i][j] for j in new_cluster])\n",
    "            if new_avg < old_avg:\n",
    "                new_cluster.append(i)\n",
    "            else:\n",
    "                in_old.append(i)\n",
    "\n",
    "        old_cluster = list(set(cluster) - set(new_cluster))\n",
    "        if old_cluster: clusters.append(old_cluster)\n",
    "        if new_cluster: clusters.append(new_cluster)\n",
    "\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac017245",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "73c2d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_to_labels(cluster_list, n_nodes):\n",
    "    labels = np.zeros(n_nodes, dtype=int)\n",
    "    for i, cluster in enumerate(cluster_list):\n",
    "        for node in cluster:\n",
    "            labels[node] = i\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c9eb6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_prepare_csv(report_filename_csv, fieldnames):\n",
    "    if os.path.exists(report_filename_csv):\n",
    "        with open(report_filename_csv, mode='r', newline='') as report_file:\n",
    "            reader = csv.reader(report_file)\n",
    "            existing_header = next(reader, None)\n",
    "\n",
    "        if existing_header == fieldnames:\n",
    "            print(\"✅ File exists. Headers match. Will start appending.\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"❌ File '{report_filename_csv}' exists but headers do not match.\\n\"\n",
    "                f\"Expected: {fieldnames}\\nFound:    {existing_header}\"\n",
    "            )\n",
    "    else:\n",
    "        with open(report_filename_csv, mode='w', newline='') as report_file:\n",
    "            writer = csv.DictWriter(report_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "        print(\"✅ File created and header written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c75bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0de99f08",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_based_cluster_sizes(n_clusters, total_nodes, entropy, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    cluster_sizes = np.ones(n_clusters, dtype=int)  # Each cluster gets at least one node\n",
    "    remaining = total_nodes - n_clusters\n",
    "\n",
    "    if entropy == 1.0:\n",
    "        # Uniform distribution\n",
    "        base = remaining // n_clusters\n",
    "        extra = remaining % n_clusters\n",
    "        cluster_sizes += base\n",
    "        for i in range(extra):\n",
    "            cluster_sizes[i] += 1\n",
    "    elif entropy == 0.0:\n",
    "        # All remaining nodes go to one randomly chosen cluster\n",
    "        idx = np.random.randint(n_clusters)\n",
    "        cluster_sizes[idx] += remaining\n",
    "    else:\n",
    "        # Mix between entropy=0 and entropy=1\n",
    "        max_alloc = int((1 - entropy) * remaining)\n",
    "        min_alloc = remaining - max_alloc\n",
    "\n",
    "        # Step 1: allocate max_alloc to one cluster\n",
    "        idx = np.random.randint(n_clusters)\n",
    "        cluster_sizes[idx] += max_alloc\n",
    "\n",
    "        # Step 2: distribute min_alloc equally among others\n",
    "        others = [i for i in range(n_clusters) if i != idx]\n",
    "        base = min_alloc // len(others)\n",
    "        extra = min_alloc % len(others)\n",
    "        for i, cluster in enumerate(others):\n",
    "            cluster_sizes[cluster] += base + (1 if i < extra else 0)\n",
    "\n",
    "    return cluster_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8f21dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_signed_graph(n_clusters=3, \n",
    "                                     total_nodes=20, \n",
    "                                     entropy=0.5,\n",
    "                                     noise_level=0.0, \n",
    "                                     seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if n_clusters > total_nodes:\n",
    "        raise ValueError(\"Number of clusters cannot exceed total number of nodes.\")\n",
    "\n",
    "    cluster_sizes = entropy_based_cluster_sizes(n_clusters=n_clusters, total_nodes=total_nodes, entropy=entropy, seed=seed)\n",
    "    \n",
    "    cluster_indices = []\n",
    "    node_id = 0\n",
    "    for size in cluster_sizes:\n",
    "        indices = list(range(node_id, node_id + size))\n",
    "        cluster_indices.append(indices)\n",
    "        node_id += size\n",
    "\n",
    "    adj_matrix = np.zeros((total_nodes, total_nodes))\n",
    "\n",
    "    for indices in cluster_indices:\n",
    "        for i in indices:\n",
    "            for j in indices:\n",
    "                if i != j:\n",
    "                    value = np.random.uniform(0.1, 1.0)\n",
    "                    adj_matrix[i, j] = value\n",
    "                    adj_matrix[j, i] = value\n",
    "\n",
    "\n",
    "    for i in range(total_nodes):\n",
    "        for j in range(total_nodes):\n",
    "            if adj_matrix[i, j] == 0 and i != j:\n",
    "                value = np.random.uniform(-1.0, -0.1)\n",
    "                adj_matrix[i, j] = value\n",
    "                adj_matrix[j, i] = value\n",
    "\n",
    "    noise = np.random.normal(0, noise_level, size=adj_matrix.shape)\n",
    "    np.fill_diagonal(adj_matrix, 0)\n",
    "    adj_matrix += noise\n",
    "    adj_matrix = np.clip(adj_matrix, -1.0, 1.0)\n",
    "\n",
    "    return adj_matrix, cluster_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c2106",
   "metadata": {},
   "source": [
    "## Clustering Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7d2ef740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signed_modularity(adj_matrix, clusters):\n",
    "    total_weight = np.sum(np.abs(adj_matrix)) / 2\n",
    "    modularity = 0\n",
    "    for cluster in clusters:\n",
    "        for i, j in combinations(cluster, 2):\n",
    "            modularity += adj_matrix[i, j]\n",
    "    return modularity / total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "52f8aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty_metric(adj_matrix, clusters):\n",
    "    penalty = 0\n",
    "    for cluster in clusters:\n",
    "        for i, j in combinations(cluster, 2):\n",
    "            if adj_matrix[i, j] < 0:  # Penalize intra-cluster negative edges\n",
    "                penalty += abs(adj_matrix[i, j])\n",
    "    for i in range(len(adj_matrix)):\n",
    "        for j in range(len(adj_matrix)):\n",
    "            if i != j and adj_matrix[i, j] > 0:  # Penalize inter-cluster positive edges\n",
    "                in_same_cluster = any(i in cluster and j in cluster for cluster in clusters)\n",
    "                if not in_same_cluster:\n",
    "                    penalty += adj_matrix[i, j]\n",
    "    return penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec065e8e",
   "metadata": {},
   "source": [
    "## Running one problem instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d263be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters=5\n",
    "total_nodes=170\n",
    "seed = 111\n",
    "\n",
    "adj_matrix, true_clusters = generate_synthetic_signed_graph(\n",
    "    n_clusters=n_clusters,\n",
    "    total_nodes=total_nodes,\n",
    "    entropy=1, \n",
    "    noise_level=0.00,\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_clusters = sorted([sorted(cluster) for cluster in true_clusters])\n",
    "print(true_clusters)\n",
    "true_labels = clusters_to_labels(true_clusters, total_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8c8bdc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters_gcsq, total_qpu_access_time, total_partition_time = gcs_q_algorithm(adj_matrix, qubo_solver='gurobi')\n",
    "# clusters_gcsq = sorted([sorted(cluster) for cluster in clusters_gcsq])\n",
    "# labels_gcsq = clusters_to_labels(clusters_gcsq, total_nodes)\n",
    "# print(\"clusters_gcsq\",clusters_gcsq)\n",
    "# print(\"gcsq\",normalized_mutual_info_score(true_labels, labels_gcsq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "30138ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0 = time.time()\n",
    "# clusters_gcsq, total_qpu_access_time, total_partition_time = gcs_q_algorithm(adj_matrix, qubo_solver='dwave')\n",
    "# tte = time.time()-t0\n",
    "# print(\"Remote\", tte)\n",
    "# print(\"Onsite\",tte-total_partition_time+total_qpu_access_time)\n",
    "# clusters_gcsq = sorted([sorted(cluster) for cluster in clusters_gcsq])\n",
    "# labels_gcsq = clusters_to_labels(clusters_gcsq, total_nodes)\n",
    "# print(\"clusters_gcsq\",clusters_gcsq)\n",
    "# print(\"gcsq\",normalized_mutual_info_score(true_labels, labels_gcsq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1cb96c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [26, 27, 28, 29]] 0\n",
      "clusters_gcsq [[0, 1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [26, 27, 28, 29]]\n",
      "gcsq 1.0\n",
      "\n",
      "clusters_pam [[0, 1, 2, 3, 4, 5, 6], [7, 15, 16, 17, 20, 21, 22, 23, 24], [8, 26, 27, 28, 29], [9, 10, 11, 12], [13, 14, 18, 19, 25]]\n",
      "pam 0.7511242502675732\n",
      "\n",
      "clusters_kmeans [[0, 1, 2, 3, 4], [5, 6, 7, 8, 27], [9, 10, 11, 12], [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [26, 28, 29]]\n",
      "kmeans 0.9458628120637362\n",
      "\n",
      "clusters_hier [[0, 1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [26, 27, 28, 29]]\n",
      "hier 1.0\n",
      "\n",
      "clusters_spec [[0, 1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [26, 27, 28, 29]]\n",
      "spec 1.0\n",
      "\n",
      "clusters_diana [[0, 1, 2, 3, 4], [5, 29], [6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]]\n",
      "diana 0.8368732493742291\n"
     ]
    }
   ],
   "source": [
    "n_clusters=5\n",
    "total_nodes=30\n",
    "seed = 42\n",
    "\n",
    "adj_matrix, true_clusters = generate_synthetic_signed_graph(\n",
    "    n_clusters=n_clusters,\n",
    "    total_nodes=total_nodes,\n",
    "    entropy=0.5, \n",
    "    noise_level=0.00,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "true_clusters = sorted([sorted(cluster) for cluster in true_clusters])\n",
    "print(true_clusters, penalty_metric(adj_matrix, true_clusters))\n",
    "true_labels = clusters_to_labels(true_clusters, total_nodes)\n",
    "\n",
    "clusters_gcsq, total_qpu_access_time, total_partition_time = gcs_q_algorithm(adj_matrix, qubo_solver='dwave')\n",
    "clusters_gcsq = sorted([sorted(cluster) for cluster in clusters_gcsq])\n",
    "labels_gcsq = clusters_to_labels(clusters_gcsq, total_nodes)\n",
    "print(\"clusters_gcsq\",clusters_gcsq)\n",
    "print(\"gcsq\",normalized_mutual_info_score(true_labels, labels_gcsq))\n",
    "\n",
    "alpha = 2\n",
    "distance_matrix = np.sqrt(alpha * (1 - adj_matrix.clip(min=-1, max=1)))\n",
    "_, clusters_pam = pam(distance_matrix, k=n_clusters)\n",
    "clusters_pam = sorted([sorted(cluster) for cluster in clusters_pam])\n",
    "labels_pam = clusters_to_labels(clusters_pam, total_nodes)\n",
    "print(\"\\nclusters_pam\",clusters_pam)\n",
    "print(\"pam\",normalized_mutual_info_score(true_labels, labels_pam))\n",
    "\n",
    "clusters_kmeans = sorted([sorted(cluster) for cluster in run_kmeans(adj_matrix, k=n_clusters, seed=42)])\n",
    "labels_kmeans = clusters_to_labels(clusters_kmeans, total_nodes)\n",
    "print(\"\\nclusters_kmeans\",clusters_kmeans)\n",
    "print(\"kmeans\",normalized_mutual_info_score(true_labels, labels_kmeans))\n",
    "\n",
    "clusters_hier = sorted([sorted(cluster) for cluster in run_hierarchical(adj_matrix, k=n_clusters)])\n",
    "labels_hier = clusters_to_labels(clusters_hier, total_nodes)\n",
    "print(\"\\nclusters_hier\",clusters_hier)\n",
    "print(\"hier\",normalized_mutual_info_score(true_labels, labels_hier))\n",
    "\n",
    "clusters_spec = sorted([sorted(cluster) for cluster in run_spectral(adj_matrix, k=n_clusters, seed=42)])\n",
    "labels_spec = clusters_to_labels(clusters_spec, total_nodes)\n",
    "print(\"\\nclusters_spec\",clusters_spec)\n",
    "print(\"spec\",normalized_mutual_info_score(true_labels, labels_spec))\n",
    "\n",
    "alpha = 2\n",
    "distance_matrix = np.sqrt(alpha * (1 - adj_matrix.clip(min=-1, max=1)))\n",
    "clusters_diana = sorted([sorted(cluster) for cluster in diana(distance_matrix, k=n_clusters)])\n",
    "labels_diana = clusters_to_labels(clusters_diana, total_nodes)\n",
    "print(\"\\nclusters_diana\",clusters_diana)\n",
    "print(\"diana\",normalized_mutual_info_score(true_labels, labels_diana))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378ea33d",
   "metadata": {},
   "source": [
    "## Experiments and saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6121ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_log_clustering_results(solvers, metrics, adj_matrix, true_labels, total_nodes, n_clusters, results, qubo_solver = 'dwave'):\n",
    "    def log_metrics(solver_name, clusters, duration):\n",
    "        labels = clusters_to_labels(clusters, total_nodes)\n",
    "        results[f\"{solver_name}_clusters\"] = clusters\n",
    "        results[f\"{solver_name}_labels\"] = labels\n",
    "        if solver_name == \"GCSQ\" and qubo_solver == \"dwave\":\n",
    "            results[f\"{solver_name}_Remote_Time\"] = duration[0]\n",
    "            results[f\"{solver_name}_Onsite_Time\"] = duration[1]\n",
    "            print(\"Remote_Time:\", round(duration[0],2), end=' ')\n",
    "            print(\"Onsite_Time:\", round(duration[1],2), end=' ')\n",
    "        else:\n",
    "            print(solver_name, round(duration,2), end=' ')\n",
    "            results[f\"{solver_name}_Time\"] = duration\n",
    "\n",
    "        if \"NMI\" in metrics:\n",
    "            print(normalized_mutual_info_score(true_labels, labels))\n",
    "            results[f\"{solver_name}_NMI\"] = normalized_mutual_info_score(true_labels, labels)\n",
    "        if \"Modularity\" in metrics:\n",
    "            results[f\"{solver_name}_Modularity\"] = signed_modularity(adj_matrix, clusters)\n",
    "        if any(m in metrics for m in [\"H\", \"C\", \"V\"]):\n",
    "            h, c, v = homogeneity_completeness_v_measure(true_labels, labels)\n",
    "            if \"H\" in metrics: results[f\"{solver_name}_H\"] = h\n",
    "            if \"C\" in metrics: results[f\"{solver_name}_C\"] = c\n",
    "            if \"V\" in metrics: results[f\"{solver_name}_V\"] = v\n",
    "        if \"FMI\" in metrics:\n",
    "            results[f\"{solver_name}_FMI\"] = fowlkes_mallows_score(true_labels, labels)\n",
    "        if \"ARI\" in metrics:\n",
    "            results[f\"{solver_name}_ARI\"] = adjusted_rand_score(true_labels, labels)\n",
    "\n",
    "\n",
    "    if \"GCSQ\" in solvers:\n",
    "        t0 = time.time()\n",
    "        clusters_gcsq, total_qpu_access_time, total_partition_time = gcs_q_algorithm(adj_matrix, qubo_solver=qubo_solver)\n",
    "        tte = time.time() - t0\n",
    "        if qubo_solver == 'dwave':\n",
    "            remote_time = tte\n",
    "            onsite_time = tte-total_partition_time+total_qpu_access_time\n",
    "            duration = [remote_time,onsite_time]\n",
    "        else:\n",
    "            duration = tte\n",
    "        clusters = sorted([sorted(cluster) for cluster in clusters_gcsq])\n",
    "        log_metrics(\"GCSQ\", clusters, duration)\n",
    "\n",
    "    if \"PAM\" in solvers:\n",
    "        alpha = 2\n",
    "        distance_matrix = np.sqrt(alpha * (1 - adj_matrix.clip(min=-1, max=1)))\n",
    "        t0 = time.time()\n",
    "        _, clusters = pam(distance_matrix, k=n_clusters)\n",
    "        duration = time.time() - t0\n",
    "        clusters = sorted([sorted(cluster) for cluster in clusters])\n",
    "        log_metrics(\"PAM\", clusters, duration)\n",
    "\n",
    "    if \"KMeans\" in solvers:\n",
    "        t0 = time.time()\n",
    "        clusters = sorted([sorted(cluster) for cluster in run_kmeans(adj_matrix, k=n_clusters, seed=42)])\n",
    "        duration = time.time() - t0\n",
    "        log_metrics(\"KMeans\", clusters, duration)\n",
    "\n",
    "    if \"Hier\" in solvers:\n",
    "        t0 = time.time()\n",
    "        clusters = sorted([sorted(cluster) for cluster in run_hierarchical(adj_matrix, k=n_clusters)])\n",
    "        duration = time.time() - t0\n",
    "        log_metrics(\"Hier\", clusters, duration)\n",
    "\n",
    "    if \"Spectral\" in solvers:\n",
    "        t0 = time.time()\n",
    "        clusters = sorted([sorted(cluster) for cluster in run_spectral(adj_matrix, k=n_clusters, seed=42)])\n",
    "        duration = time.time() - t0\n",
    "        log_metrics(\"Spectral\", clusters, duration)\n",
    "\n",
    "    if \"DIANA\" in solvers:\n",
    "        t0 = time.time()\n",
    "        alpha = 2\n",
    "        distance_matrix = np.sqrt(alpha * (1 - adj_matrix.clip(min=-1, max=1)))\n",
    "        clusters = sorted([sorted(cluster) for cluster in diana(distance_matrix, k=n_clusters)])\n",
    "        duration = time.time() - t0\n",
    "        log_metrics(\"DIANA\", clusters, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2bf9da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File exists. Headers match. Will start appending.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "solvers = [\"GCSQ\", \"PAM\", \"KMeans\", \"Hier\", \"Spectral\"]\n",
    "solvers = [\"GCSQ\"]\n",
    "\n",
    "qubo_solver = \"dwave\"\n",
    "\n",
    "metrics = [\"clusters\", \"labels\", \"NMI\", \"Modularity\", \"H\", \"C\", \"V\", \"FMI\", \"ARI\"]\n",
    "\n",
    "fieldnames = [\"seed\", \"total_nodes\", \"n_clusters\", \"entropy\", \"true_clusters\", \"true_labels\"]\n",
    "\n",
    "\n",
    "if \"GCSQ\" in solvers:\n",
    "    fieldnames += [f\"GCSQ_{metric}\" for metric in metrics]\n",
    "    if qubo_solver == \"dwave\":\n",
    "        fieldnames += [\"GCSQ_Remote_Time\", \"GCSQ_Onsite_Time\"]\n",
    "    else:\n",
    "        fieldnames += [\"GCSQ_Time\"]\n",
    "\n",
    "for solver in [s for s in solvers if s != \"GCSQ\"]:\n",
    "    for metric in metrics + [\"Time\"]:\n",
    "        fieldnames.append(f\"{solver}_{metric}\")\n",
    "\n",
    "report_filename_csv = f\"report_quantum.csv\"\n",
    "\n",
    "check_and_prepare_csv(report_filename_csv, fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db381347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73919b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = np.arange(10,171,10).tolist()\n",
    "noise_level=0.00\n",
    "seeds=[111,222,333,444,555]\n",
    "entropy_list = [1.0, 0.75, 0.5, 0.25, 0.0]\n",
    "entropy_list = [1.0, 0.5, 0.0]\n",
    "\n",
    "for seed in seeds:\n",
    "    for total_nodes in node_list:\n",
    "        # print(f\"Seed: {seed}, total_nodes: {total_nodes}\")\n",
    "        n_clusters_list = [5, 10, 15, 20]\n",
    "        for n_clusters in n_clusters_list:\n",
    "            if n_clusters > total_nodes:\n",
    "                continue\n",
    "            for entropy in entropy_list:\n",
    "                print(f\"Seed: {seed}, total_nodes: {total_nodes}, n_clusters: {n_clusters}\")\n",
    "                results = {}\n",
    "                results[\"seed\"] = seed\n",
    "                results[\"total_nodes\"] = total_nodes\n",
    "                results[\"n_clusters\"] = n_clusters\n",
    "                results[\"entropy\"] = entropy\n",
    "    \n",
    "                adj_matrix, true_clusters = generate_synthetic_signed_graph(n_clusters=n_clusters,\n",
    "                                                                             total_nodes=total_nodes,\n",
    "                                                                             entropy=entropy,\n",
    "                                                                             noise_level=noise_level,\n",
    "                                                                             seed=seed)\n",
    "                true_labels = clusters_to_labels(true_clusters, total_nodes)\n",
    "                results[\"true_clusters\"] = true_clusters\n",
    "                results[\"true_labels\"] = true_labels\n",
    "    \n",
    "                evaluate_and_log_clustering_results(\n",
    "                    solvers=solvers,\n",
    "                    metrics=metrics,\n",
    "                    adj_matrix=adj_matrix,\n",
    "                    true_labels=true_labels,\n",
    "                    total_nodes=total_nodes,\n",
    "                    n_clusters=n_clusters,\n",
    "                    results=results,\n",
    "                    qubo_solver = qubo_solver\n",
    "                )\n",
    "    \n",
    "                with open(report_filename_csv, mode='a', newline='') as report_file:\n",
    "                    writer = csv.DictWriter(report_file, fieldnames=fieldnames)\n",
    "                    row = {key: results.get(key, None) for key in fieldnames}\n",
    "                    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ecee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
